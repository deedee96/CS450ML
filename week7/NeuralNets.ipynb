{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Need to check if i have added the node to the list again\n",
    "\n",
    "\n",
    "#Node for the nerwork\n",
    "class Node:\n",
    "    def __init__(self, nClasses):\n",
    "        self.nClasses = nClasses\n",
    "        self.hx = 0\n",
    "        self.gx = 0\n",
    "        self.gama = 0\n",
    "        #self.weights = np.random.randn(nClasses + 1)\n",
    "        self.weights = []\n",
    "            \n",
    "    \n",
    "    def createWeights(self):\n",
    "        for i in xrange(self.nClasses + 1):\n",
    "            self.weights.append(float(\"%.2f\" % np.random.uniform(-1,1.1)))\n",
    "            \n",
    "    #set the error value for node        \n",
    "    def setErr(self, gama):\n",
    "        self.gama = gama\n",
    "    \n",
    "    def getErr(self):\n",
    "        return self.gama\n",
    "    \n",
    "    def setHx(self, hx):\n",
    "        self.hx = hx\n",
    "        \n",
    "    def getGx(self): \n",
    "        return self.gx\n",
    "    \n",
    "    def getWeight(self, index):\n",
    "        return self.weights[index]\n",
    "    \n",
    "    def setWeight(self, index, weight):\n",
    "        self.weights[index] = weight\n",
    "    \n",
    "    def calGx(self):\n",
    "        self.gx = 1 / (1 + np.exp(-self.hx))\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, nNodes,classes):\n",
    "        self.nNodes = nNodes\n",
    "        self.node_list = []\n",
    "        \n",
    "        for i in xrange(self.nNodes): \n",
    "            self.node_list.append(Node(len(classes)));\n",
    "        \n",
    "    def feedForward(self, classes):\n",
    "        self.data = []\n",
    "        count = 0\n",
    "        for node in self.node_list:\n",
    "            node.createWeights()\n",
    "            #bias node\n",
    "            hx = -1 * node.weights[0]\n",
    "            for i in xrange(len(classes)):\n",
    "                temp = classes[0] * node.weights[i+1]\n",
    "                hx += temp\n",
    "            node.setHx(hx)\n",
    "            node.calGx()\n",
    "            self.node_list[count] = node\n",
    "            self.data.append(node.getGx())\n",
    "            count += 1\n",
    "            \n",
    "    def getNodeList(self):\n",
    "        return self.node_list\n",
    "            \n",
    "    def getnNode(self):\n",
    "        return self.nNodes\n",
    "    \n",
    "    def setNode(self,index, node):\n",
    "        self.node_list[index] = node\n",
    "    \n",
    "    def getNode(self, index):\n",
    "        return self.node_list[index]\n",
    "    \n",
    "    def getData(self):\n",
    "        #return the feedfoward list\n",
    "        return self.data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self, nLayers = 2):\n",
    "        self.nLayers = nLayers\n",
    "        #list of layers for the whole training data\n",
    "        self.Layers = []\n",
    "\n",
    "                \n",
    "    def fit(self, train_data, train_target):\n",
    "        self.train_target = train_target\n",
    "        self.train_data = train_data\n",
    "        for i in xrange(self.nLayers - 1):\n",
    "            temp = Layer(3, train_data[0])\n",
    "            temp.feedForward(train_data[0])\n",
    "            self.Layers.append(temp)\n",
    "        target_classes = len(set(self.train_target))\n",
    "        if target_classes == 2:\n",
    "            output_layer = Layer(1, self.Layers[0].getData())\n",
    "        else:\n",
    "            output_layer = Layer(target_classes, self.Layers[0].getData())\n",
    "        output_layer.feedForward(self.Layers[0].getData())\n",
    "        self.Layers.append(output_layer)\n",
    "\n",
    "\n",
    "        \n",
    "    def train(self):\n",
    "        learning_rate = 0.1\n",
    "        train_list = []\n",
    "        counter = 0\n",
    "        target_classes = len(set(self.train_target))\n",
    "\n",
    "        for row in self.train_data:\n",
    "            self.Layers[0].feedForward(row)\n",
    "            self.Layers[1].feedForward(self.Layers[0].getData())\n",
    "            \n",
    "            if target_classes == 2:\n",
    "                fire = self.Layers[1].getData()[0]\n",
    "                if (fire >= .5):\n",
    "                    train_list.append(1)\n",
    "                else:\n",
    "                    train_list.append(0)\n",
    "                    \n",
    "            else:\n",
    "                fire = np.argmax(self.Layers[-1].getData())\n",
    "                #print fire\n",
    "                train_list.append(fire)\n",
    "            \n",
    "            \n",
    "            #calculate error for the output layer\n",
    "            ol_index = 0\n",
    "            _node_runner = 0\n",
    "            for node in self.Layers[-1].getNodeList():\n",
    "                #if there the target is binary:\n",
    "                if (target_classes == 2):\n",
    "                    error = node.getGx() * (1 - node.getGx()) * (node.getGx() - self.train_target[counter])\n",
    "            \n",
    "                #other case\n",
    "                else:\n",
    "                    if ol_index == self.train_target[counter]:\n",
    "                        error = node.getGx() * (1 - node.getGx()) * (node.getGx() - 1)\n",
    "                    else:\n",
    "                        error = node.getGx() * (1 - node.getGx()) * (node.getGx() - 0)\n",
    "                    ol_index += 1\n",
    "                node.setErr(error)\n",
    "                self.Layers[-1].setNode(_node_runner, node)\n",
    "                _node_runner += 1\n",
    "\n",
    "            index = 1\n",
    "            _node_runner = 0\n",
    "            for node in self.Layers[0].getNodeList():\n",
    "                error = node.getGx() * (1 - node.getGx())\n",
    "                _sum = 0\n",
    "                for node_k in self.Layers[1].getNodeList():\n",
    "                    _sum += (node_k.getWeight(index) * node_k.getErr())\n",
    "                error *= _sum\n",
    "                node.setErr(error)\n",
    "                self.Layers[0].setNode(_node_runner, node)\n",
    "                _node_runner += 1\n",
    "                index += 1\n",
    "            \n",
    "            #setting weight for the bias\n",
    "            _node_runner = 0\n",
    "            for node in self.Layers[1].getNodeList():\n",
    "                newWeight = node.getWeight(0) - learning_rate * node.getErr() * -1\n",
    "                node.setWeight(0, newWeight)\n",
    "                self.Layers[1].setNode(_node_runner, node)\n",
    "                _node_runner += 1\n",
    "\n",
    "            _node_runner = 0\n",
    "            for node in self.Layers[0].getNodeList():\n",
    "                newWeight = node.getWeight(0) - learning_rate * node.getErr() * -1\n",
    "                node.setWeight(0, newWeight)   \n",
    "                self.Layers[0].setNode(_node_runner, node)\n",
    "                _node_runner += 1\n",
    "            #setting for the others\n",
    "           \n",
    "            for i in xrange(self.Layers[0].getnNode()):\n",
    "                _node_runner = 0\n",
    "                for node in self.Layers[1].getNodeList():\n",
    "                    newWeight = node.getWeight(i + 1) - learning_rate * node.getErr() * self.Layers[0].getNode(i).getGx()\n",
    "                    node.setWeight(i + 1, newWeight)\n",
    "                    self.Layers[1].setNode(_node_runner, node)\n",
    "                    _node_runner += 1\n",
    "                    \n",
    "                  \n",
    "            for i in xrange(len(row)):\n",
    "                _node_runner = 0  \n",
    "                for node in self.Layers[0].getNodeList():\n",
    "                    newWeight = node.getWeight(i + 1) - learning_rate * node.getErr() * row[i]\n",
    "                    node.setWeight(i + 1, newWeight)\n",
    "                    self.Layers[0].setNode(_node_runner, node)\n",
    "                    _node_runner += 1\n",
    "            counter += 1\n",
    "        match = 0\n",
    "        for i in xrange(len(train_list)):\n",
    "            if train_list[i] == self.train_target[i]:\n",
    "                match += 1\n",
    "                \n",
    "        \n",
    "        return (\"%.2f\" % (match / len(self.train_target) * 100));    \n",
    "             \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=0)\n",
    "X_norm = sklearn.preprocessing.normalize(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "netWork = NeuralNet()\n",
    "netWork.fit(X_norm, y_train)\n",
    "accuracy_list = []\n",
    "for i in xrange(200):\n",
    "    accuracy =  netWork.train()\n",
    "    accuracy_list.append(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['33.33',\n",
       " '33.33',\n",
       " '34.29',\n",
       " '36.19',\n",
       " '36.19',\n",
       " '36.19',\n",
       " '36.19',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14',\n",
       " '37.14']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/LamaHamadeh/Pima-Indians-Diabetes-DataSet-UCI/master/pima_indians_diabetes.txt\",header=None,skipinitialspace=True)\n",
    "df[[1,2,3,4,5]] = df[[1,2,3,4,5]].replace(0, np.NAN)\n",
    "df.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "values = df.values\n",
    "X = values[:,0:8]\n",
    "y = values[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "X_norm = sklearn.preprocessing.normalize(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88',\n",
       " '67.88']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netWork = NeuralNet()\n",
    "netWork.fit(X_norm, y_train)\n",
    "accuracy_list = []\n",
    "for i in xrange(200):\n",
    "    accuracy =  netWork.train()\n",
    "    accuracy_list.append(accuracy)\n",
    "accuracy_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
